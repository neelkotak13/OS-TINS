import feedparser, time, os, trafilatura, sys
from datetime import datetime, date, timedelta
from collections import defaultdict
from datetime import datetime 
from google import genai

# API key being read from api_config.py file variable set (ensure is included in .gitignore if pushing code via git)
from api_config import GEMINI_API_KEY
## GLOBAL VARIABLES BELOW -- ENSURE CORRECT VALUES ARE SET
####################################
# Default location for text files with links to RSS feeds from sites, keep each URL on a separate line
FEEDS_PATH  = 'rss_feed_links.txt'

# Start date, updates the days value if you want to parse older articles available on the RSS feed
START_DATE = date.today() - timedelta(days=2)

# End date of articles to complete parsing (set to same day as start date if only for a single day)
END_DATE = date.today()

# Disclaimer to be added to each created file to ensure the user is aware of content being generated by LLM
DISCLAIMER_TEXT = "DISCLAIMER: The summary above was generated by an artificial intelligence Large Language Model (LLM). While LLMs strive for accuracy and quality, please note that the information provided may not be entirely error-free or up-to-date. We recommend independently verifying the content and consulting with professionals for specific advice or information. We do not assume any responsibility or liability for the use of interpretation of this content."

####################################
## END GLOBAL VARIABLES

# This function will parse the FEEDS_PATH designated file reading each line as a separate URL for RSS feeds
# By default this will contain the sites bleepingcomputer, darkreading, and krebsonsecurity rss links
def parse_feed_file() -> list:
    # empty list to add feeds to
    feeds = list()
    # process feeds in FEEDS_PATH variable
    with open(FEEDS_PATH, 'r') as feed_file:
        for rss_url in feed_file:
            feeds.append(rss_url.rstrip())
    return feeds

# This function takes the feed links list returned from parse_feed_file()
# It will parse the XML and return links to each individual story
# Will return a dictionary where key is a tuple of (website, article_title, date_published) and value is the list of articles published on that date 
def parse_feeds(feeds) -> dict():
    # dictionary to return all links published on date
    datePublished_articles_dictionary = defaultdict(list)
    # for each feed, read the content into feedparser
    for rss_url in feeds:
        # parse through each individual XML channel for entries
        xml_channel = feedparser.parse(rss_url)
        # individual entry for each article to parse through
        for entry in xml_channel.entries:
            # if feed publish time is in alignment with global variable ranges, then add to dictionary
            local_time_feed_published = time.localtime(time.mktime(entry.updated_parsed))
            entry_date = datetime(*local_time_feed_published[:3]).date()
            if (START_DATE <= entry_date) and (END_DATE >= entry_date):
                datePublished_articles_dictionary[(xml_channel.channel.title, entry.title, entry_date)].append(entry.link.removeprefix("https://"))

    return datePublished_articles_dictionary

# This function will download all the articles passed by a dictionary with the keys as date published and values as article links
# The files will be downloaded into a nested folder called 'articles' with subdirectories being the date the article was published
# and think article links as the file names
def download_articles(parsed_entries: dict):
    # return dictionary with tuple of strings (website name, url) as key and string of local path of extracted text written to as value (ex: {('BleepingComputer', 'www.bleepingcomputer.com/path/to/article'} : '/home/user/localpath/to/article'])
    extracted_articles = {}
    # creates local directory 'articles' to download files into, sorted by date
    for article_data, urls in parsed_entries.items():
        website_name = article_data[0]
        article_name = article_data[1]
        article_date = article_data[2]
        folder_path = f"articles/{website_name}/{article_date}"
        for url in urls:
            # retrieve url content and extract only text from article
            raw_html = trafilatura.fetch_url(url)
            article_content = trafilatura.extract(raw_html)
            os.makedirs(folder_path,exist_ok=True)
            try:
                with open(f"{folder_path}/{article_name}.txt", "w", encoding='utf-8') as f:
                    # make folder & store extracted text to that file named after article title within
                    f.writelines(article_content)
                    extracted_articles[(website_name, url)] = f"{os.getcwd()}/{folder_path}/{article_name}.txt"
                    print(f"Wrote extracted article content to '{os.getcwd()}/{folder_path}/{article_name}.txt' from {website_name}")
            except TypeError:
                os.remove(f"{folder_path}/{article_name}.txt")
                print(url + ' is empty when parsed. Unable to write extracted content')
            except FileNotFoundError:
                print(url + ' did not download article for some reason, moving on')
            except OSError:
                print(url + ' some kind of filename formatting issue')

    return extracted_articles

def gemini_summarize_articles(article_path, article_url):
    with open(f"{article_path}", "r", encoding='utf-8') as f:
        article_content = f.read()
        # Query Gemini via API
        client = genai.Client(api_key=GEMINI_API_KEY)
        sys_instruct = "I extracted the following data from an online news article. I want you to provide a concise summary of the information to a working cybersecurity professional that will receive this in a daily newsletter. Their main concerns would be centered around if their organization would be affected by this event. Be sure to include how the attack was discovered or to whom it was reported to, if applicable and known. When mentioning organizations or companies, include their industry or purpose. Provide information as accurate as possible, include 2-3 IoCs/TTPs at the very end of the summary. These summaries must be between 1 to 7 sentences in length to keep it as concise as possible. Ensure to use active voice when creating the summary and if you are unsure of the certainty of any statements, please state so in the summary. Lastly, finish off with speculation of potentially impacted parties. Here are some examples of what the summaries should look like: ```Qilin ransomware has claimed responsibility for the February 3rd cyberattack on Lee Enterprises, a US-based media company, and leaked samples of allegedly stolen data after Lee Enterprise's disclosure to the SEC. The attackers claim to have stolen 350GB of data, including sensitive documents, and threaten to release it all on March 5, 2025, if a ransom is not paid. Lee Enterprises is aware of the claims and is currently investigating. Qilin has been known to target organizations such as automotive giant Yangfeng, Australia's Court Services Victoria, and NHS hospitals in London and is associated with the Scattered Spider hacking group. Organizations in the media and publishing sectors should assess their risk and review security measures. IoCs/TTPs: Data exfiltration, encryption of critical applications, custom Chrome credential stealer.```",
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=["Summarize this article text:```" + article_content + "```"] 
        )

    try:
        with open(f"{article_path}_summary.md", "w", encoding='utf-8') as f:
            # make folder & store extracted text to that file named after article title within
            f.writelines(response.text)
            f.writelines("\nOriginal article URL: " + article_url)
            f.writelines(f"\n\n{DISCLAIMER_TEXT}\n")
            print(f"Wrote LLM summary to '{article_path}_summary.md'")
    except Exception as e:
        os.remove(f"{article_path}_summary.md")
        print(e)


    return f"{article_path}_summary.md"

def main():
    # check if any args were provided for testing directly on a list of URLs as an argument
    if len(sys.argv) > 1:
        # iterate through URLs (excluding the script name)
        for i, url in enumerate(sys.argv[1:]):
            article_name = "individual_articles/"
            # make initial individual article folder if doesn't exist already
            os.makedirs(article_name,exist_ok=True)
            # takes the last non-empty directory as local extracted text file name 
            url_split = url.split('/')
            for article in url_split[::-1]:
                if article != '':
                    article_name+=article
                    article_name+=".txt"
                    break
            # retrieve url content and extract only text from article
            raw_html = trafilatura.fetch_url(url)
            article_content = trafilatura.extract(raw_html)
            try:
                with open(f"{article_name}", "w", encoding='utf-8') as f:
                    # make folder & store extracted text to that file named after article title within
                    f.writelines(article_content)
                    # extracted_articles[(website_name, url)] = f"{article_path}.txt"
                    print(f"Wrote extracted article content to '{article_name}'\n")
            except TypeError:
                os.remove(f"{article_name}")
                print(url + ' is empty when parsed. Unable to write extracted content')
            except FileNotFoundError:
                print(url + ' did not download article for some reason, moving on')
            except OSError:
                print(url + ' some kind of filename formatting issue')

            print(f"Sending {url} article {article_name} to Gemini for summary\n")
            summary_name = gemini_summarize_articles(article_name, url)
            print(f"AI summary for {url} written to {summary_name}\n")

    # if no URL args are passed, simply read RSS file 
    else:
        feeds = parse_feed_file()
        # take rss feed URLs and go through to extract individual articles/blogs/news links
        # Will return a dictionary where  
        parsed_entries = parse_feeds(feeds) # key is a tuple of (website, article_title, date_published) and value is the list of articles published on that dates
        # locally download article content that fit in date range from RSS links, return paths in articles folder sorted by date
        article_paths = download_articles(parsed_entries)
        # send each article individually to gemini to summarize
        for (website, url), article in article_paths.items():
            print(f"Sending {website} article {article} to Gemini for summary\n")
            summary_path = gemini_summarize_articles(article, url)
            print(f"AI summary for {article} written to {summary_path}\n")

    return 0

if __name__ == "__main__":
    main()